{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c42b24308ac282",
   "metadata": {},
   "source": [
    "# Prepare the Environment"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## To prepare the environment, uncomment the below cell and run.",
   "id": "acd9785ad50c0371"
  },
  {
   "cell_type": "code",
   "id": "5feabc41f117c737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T04:21:08.606711Z",
     "start_time": "2024-11-19T04:21:08.600434Z"
    }
   },
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install tensorflow\n",
    "# \n",
    "# !pip install datasets\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn\n",
    "# !pip install --upgrade ipywidgets\n",
    "# !pip install --upgrade jupyter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "36b0bcc108df5f17",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "    Prerequisites:\n",
    "    Basic Python programming\n",
    "    Understanding of text processing techniques\n",
    "    Tasks:\n",
    "    Load the BEAD dataset, focusing on the \"1-Text-Classification\" folder.\n",
    "    Implement text cleaning functions:\n",
    "    Split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fae3bbc3fc5cf",
   "metadata": {},
   "source": [
    "### Task 1: Load the BEAD dataset, focusing on the \"1-Text-Classification\" folder."
   ]
  },
  {
   "cell_type": "code",
   "id": "11eace93087d72b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T04:21:23.735609Z",
     "start_time": "2024-11-19T04:21:10.804354Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"shainar/BEAD\", \"3-Aspects\")\n",
    "\n",
    "# Load the bias training data.\n",
    "splits = {'bias_train': '1-Text-Classification/bias-train.csv', 'bias_valid': '1-Text-Classification/bias-valid.csv', 'bias_train_instruction': '1-Text-Classification/bias-train_10k-instruction-data.csv', 'bias_valid_instruction': '1-Text-Classification/bias-valid_1k-instruction-data.csv', 'sentiment_train': '1-Text-Classification/sentiment-train.csv', 'sentiment_valid': '1-Text-Classification/sentiment-valid.csv', 'toxic_train': '1-Text-Classification/toxic-train.csv', 'toxic_valid': '1-Text-Classification/toxic-valid.csv', 'multi_label': '1-Text-Classification/multi-label.csv'}\n",
    "\n",
    "# splits_3 = {'bias_tokens': '2-Token-Classification/Bias_tokens.csv', 'conll': '2-Token-Classification/conll.csv', 'conll_bias': '2-Token-Classification/bias-conll.csv'}\n",
    "\n",
    "# df_1 = pd.read_csv(\"hf://datasets/shainar/BEAD/0-Full Annotations/Full.csv\")\n",
    "\n",
    "# Read the bias training data\n",
    "df = pd.read_csv(\"hf://datasets/shainar/BEAD/\" + splits[\"bias_train\"])\n",
    "\n",
    "# df_3 = pd.read_csv(\"hf://datasets/shainar/BEAD/\" + splits_3[\"bias_tokens\"])\n",
    "\n",
    "# print(ds.keys())  # This will show you all available splits like 'train', 'validation', etc.\n",
    "\n",
    "ds_cleaned = ds['aspects'].to_pandas().copy()\n",
    "#df_cleaned = df.copy()\n",
    "\n",
    "# Preview the data\n",
    "print(\"Dataset 1: \")\n",
    "\n",
    "print(ds_cleaned.head()) \n",
    "\n",
    "#print(\"Dataset 2: \")\n",
    "#print(df_cleaned.head())\n",
    "\n",
    "# print(\"Dataset 3: \")\n",
    "# print(df_3.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: \n",
      "                                                text   Aspect\n",
      "0  why does everyone always have to end up leavin...  abandon\n",
      "1  Get out of a sick bed, log onto Twitter and se...  abandon\n",
      "2          I've been abandoned with nothing to eat.   abandon\n",
      "3  @rustyrockets I'm not being funny, but where a...  abandon\n",
      "4        i'm all alone because @smt90210 left me...   abandon\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "3ed1c36e3a2cbb60",
   "metadata": {},
   "source": [
    "### Task 2: Implement text cleaning functions & Split the datasets into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ff2df982ac631c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T04:26:26.777332Z",
     "start_time": "2024-11-19T04:25:43.430130Z"
    }
   },
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace all non-alphabetic characters (punctuation, numbers, etc.) with spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize and remove stopwords using sklearn stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # Remove single-letter words\n",
    "    filtered_words = [word for word in filtered_words if len(word) > 1]\n",
    "    \n",
    "    # Rejoin the filtered words into a cleaned-up string\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Remove rows where 'text' column is empty or contains only whitespace\n",
    "ds_cleaned = ds_cleaned[ds_cleaned['text'].str.strip().astype(bool)]\n",
    "#df_cleaned = df_cleaned[df_cleaned['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Remove rows where 'label' column is not 0 or 1\n",
    "#df_cleaned = df_cleaned[df_cleaned['label'].isin([0, 1])]\n",
    "\n",
    "# Apply the text cleaning function to the 'text' column\n",
    "ds_cleaned['processed_text'] = ds_cleaned['text'].apply(clean_text)\n",
    "#df_cleaned['processed_text'] = df_cleaned['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the cleaned text using NLTK's word_tokenize\n",
    "ds_cleaned['tokens'] = ds_cleaned['processed_text'].apply(word_tokenize)\n",
    "#df_cleaned['tokens'] = df_cleaned['processed_text'].apply(word_tokenize)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"Processed Data_1:\", ds_cleaned.head())\n",
    "#print(\"Processed Data_2:\", df_cleaned.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data_1:                                                 text   Aspect  \\\n",
      "0  why does everyone always have to end up leavin...  abandon   \n",
      "1  Get out of a sick bed, log onto Twitter and se...  abandon   \n",
      "2          I've been abandoned with nothing to eat.   abandon   \n",
      "3  @rustyrockets I'm not being funny, but where a...  abandon   \n",
      "4        i'm all alone because @smt90210 left me...   abandon   \n",
      "\n",
      "                                      processed_text  \\\n",
      "0                                   does end leaving   \n",
      "1  sick bed log twitter ve purged abandoned twitt...   \n",
      "2                                   ve abandoned eat   \n",
      "3    rustyrockets funny abandoned fair sigh xxxxxxxx   \n",
      "4                                           smt left   \n",
      "\n",
      "                                              tokens  \n",
      "0                               [does, end, leaving]  \n",
      "1  [sick, bed, log, twitter, ve, purged, abandone...  \n",
      "2                               [ve, abandoned, eat]  \n",
      "3  [rustyrockets, funny, abandoned, fair, sigh, x...  \n",
      "4                                        [smt, left]  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4ebf305aa9b35a0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-19T04:26:58.984749Z"
    }
   },
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the list of valid English words\n",
    "valid_words = set(words.words())\n",
    "\n",
    "# Convert NLTK's part-of-speech tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Check if the word is a valid word in the vocabulary\n",
    "def is_valid_word(word):\n",
    "    return word in valid_words or wordnet.synsets(word)\n",
    "\n",
    "# Define a function to perform part-of-speech tagging, lemmatization, and validity check\n",
    "def lemmatize_tokens(tokens):\n",
    "    pos_tagged = pos_tag(tokens)  # POS tagging for each word\n",
    "    lemmatized = []\n",
    "    for token, tag in pos_tagged:\n",
    "        # Lemmatize the word based on its POS tag\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "        # Check if the lemmatized word is valid\n",
    "        if is_valid_word(lemma):\n",
    "            lemmatized.append(lemma)\n",
    "        else:\n",
    "            lemmatized.append(\"XXXXX\")  # Use \"XXXXX\" to mark invalid words\n",
    "    return [word for word in lemmatized if word != \"XXXXX\"]  # Remove \"XXXXX\" words\n",
    "\n",
    "# Apply lemmatization to the 'tokens' column of each row\n",
    "ds_cleaned['lemmatized_tokens'] = ds_cleaned['tokens'].apply(lemmatize_tokens)\n",
    "#df_cleaned['lemmatized_tokens'] = df_cleaned['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Display the processed data\n",
    "print(ds_cleaned[['processed_text', 'tokens', 'lemmatized_tokens']].head())\n",
    "#print(df_cleaned[['processed_text', 'tokens', 'lemmatized_tokens']].head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "227371d80bf12059",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate datasets for ds_cleaned\n",
    "X_ds = ds_cleaned['lemmatized_tokens']\n",
    "y_ds = ds_cleaned['Aspect']\n",
    "X_train_ds, X_test_ds, y_train_ds, y_test_ds = train_test_split(X_ds, y_ds, test_size=0.2, random_state=42)\n",
    "train_ds = pd.DataFrame({'lemmatized_tokens': X_train_ds, 'Aspect': y_train_ds})\n",
    "test_ds = pd.DataFrame({'lemmatized_tokens': X_test_ds, 'Aspect': y_test_ds})\n",
    "\n",
    "# # Separate datasets for df_cleaned\n",
    "# X_df = df_cleaned['lemmatized_tokens']  # Features\n",
    "# y_df = df_cleaned['label']              # Labels (assume there is a label column)\n",
    "# X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "# train_df = pd.DataFrame({'lemmatized_tokens': X_train_df, 'label': y_train_df})\n",
    "# test_df = pd.DataFrame({'lemmatized_tokens': X_test_df, 'label': y_test_df})\n",
    "\n",
    "\n",
    "train_size_1 = len(train_ds)\n",
    "test_size_1 = len(test_ds)\n",
    "total_size_1 = len(train_ds) + len(test_ds)\n",
    "\n",
    "# train_size_2 = len(train_df)\n",
    "# test_size_2 = len(test_df)\n",
    "# total_size_2 = len(train_df) + len(test_df)\n",
    "\n",
    "# View the split datasets\n",
    "print(\"Dataset DS:\")\n",
    "print(f\"Number of Training Data Size: {train_size_1}\")\n",
    "print(f\"Percentage: {(train_size_1 / total_size_1):.2f}\")\n",
    "print(f\"Number of Test Data Size: {test_size_1}\")\n",
    "print(f\"Percentage: {(test_size_1 / total_size_1):.2f}\")\n",
    "# print()\n",
    "# print(\"Dataset DF:\")\n",
    "# print(f\"Number of Training Data Size: {train_size_2}\")\n",
    "# print(f\"Percentage: {(train_size_2 / total_size_2):.2f}\")\n",
    "# print(f\"Number of Test Data Size: {test_size_2}\")\n",
    "# print(f\"Percentage: {(test_size_2 / total_size_2):.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7691d9e1571a1351",
   "metadata": {},
   "source": [
    "# Step 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82020ba28d0a9e1",
   "metadata": {},
   "source": [
    "### Task 1: Implement TF-IDF vectorization on the cleaned text data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom TF-IDF",
   "id": "9aab1d11ad5fc44d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Compute Term Frequency (TF) for a row of tokens\n",
    "def compute_term_frequency(row_tokens):\n",
    "    term_frequency = Counter(row_tokens)  # Count occurrences of each word\n",
    "    total_token_count = len(row_tokens)  # Count total number of words in the row\n",
    "    # Compute the relative frequency of each word (Term Frequency)\n",
    "    term_frequency = {word: count / total_token_count for word, count in term_frequency.items()}\n",
    "    return term_frequency \n",
    "\n",
    "# Compute Inverse Document Frequency (IDF) for the entire corpus\n",
    "def compute_inverse_document_frequency(corpus):\n",
    "    number_of_rows = len(corpus)\n",
    "    row_frequency = Counter()  # Record how many rows each word appears in\n",
    "\n",
    "    # Loop over each row in the corpus\n",
    "    for row_tokens in corpus:\n",
    "        unique_tokens_in_row = set(row_tokens)  # Get unique words in the row\n",
    "        # Count each word that appears at least once in the row\n",
    "        for word in unique_tokens_in_row:\n",
    "            row_frequency[word] += 1\n",
    "\n",
    "    # Compute Inverse Document Frequency for each word\n",
    "    inverse_document_frequency = {\n",
    "        word: math.log(number_of_rows / (1 + row_count)) \n",
    "        for word, row_count in row_frequency.items()\n",
    "    }\n",
    "    return inverse_document_frequency\n",
    "\n",
    "# Compute Term Frequency-Inverse Document Frequency (TF-IDF) for the entire corpus\n",
    "def compute_term_frequency_inverse_document_frequency(corpus):\n",
    "    # Compute IDF for the whole corpus\n",
    "    inverse_document_frequency = compute_inverse_document_frequency(corpus)\n",
    "    \n",
    "    tfidf_scores_for_rows = []\n",
    "    # Loop over each row in the corpus\n",
    "    for row_tokens in corpus:\n",
    "        # Compute the term frequency for the row\n",
    "        term_frequency = compute_term_frequency(row_tokens)\n",
    "        # Compute TF-IDF for each word in the row\n",
    "        tfidf = {\n",
    "            word: term_frequency.get(word, 0) * inverse_document_frequency.get(word, 0)\n",
    "            for word in term_frequency\n",
    "        }\n",
    "        tfidf_scores_for_rows.append(tfidf)\n",
    "    \n",
    "    return tfidf_scores_for_rows"
   ],
   "id": "436d3fedb8e7dae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fed38b721d98e8f0",
   "metadata": {},
   "source": [
    "# Example usage on ds_cleaned (assuming it's a Hugging Face Dataset)\n",
    "corpus_ds_cleaned = ds_cleaned['lemmatized_tokens']  # This will be the list of lemmatized tokens for each row\n",
    "tfidf_scores_ds_cleaned = compute_term_frequency_inverse_document_frequency(corpus_ds_cleaned)\n",
    "\n",
    "# # Example usage on df_cleaned (assuming it's a pandas DataFrame)\n",
    "# corpus_df_cleaned = df_cleaned['lemmatized_tokens'].tolist()  # Convert the column to a list of lists\n",
    "# tfidf_scores_df_cleaned = compute_term_frequency_inverse_document_frequency(corpus_df_cleaned)\n",
    "\n",
    "# Print results for verification (example for the first few rows)\n",
    "print(\"TF-IDF scores for ds_cleaned (first row):\")\n",
    "print(tfidf_scores_ds_cleaned[0])\n",
    "print(\"TF-IDF scores for ds_cleaned (second row):\")\n",
    "print(tfidf_scores_ds_cleaned[1])\n",
    "print(\"TF-IDF scores for ds_cleaned (third row):\")\n",
    "print(tfidf_scores_ds_cleaned[2])\n",
    "\n",
    "\n",
    "# print(\"TF-IDF scores for df_cleaned (first row):\")\n",
    "# print(tfidf_scores_df_cleaned[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27d6e8f9166a4686",
   "metadata": {},
   "source": "### (OPTIONAL)Task 2: Explore word embedding techniques (e.g., Word2Vec or GloVe) for more advanced feature representation."
  },
  {
   "cell_type": "code",
   "id": "9fa9fbc42df3deb0",
   "metadata": {},
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# \n",
    "# # Example: Train Word2Vec model on your lemmatized tokens (list of tokens)\n",
    "# model = Word2Vec(df_cleaned['lemmatized_tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "# \n",
    "# # Save the model for later use\n",
    "# model.save(\"word2vec_model.model\")\n",
    "# \n",
    "# # Example: Find the vector for a word\n",
    "# word_vector = model.wv['good']  # Replace 'example' with any word in your dataset\n",
    "# \n",
    "# # Find similar words\n",
    "# similar_words = model.wv.most_similar('good', topn=10)\n",
    "# print(similar_words)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e67a7e553630c28",
   "metadata": {},
   "source": [
    "# Step 3: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08211d4a7074ed",
   "metadata": {},
   "source": [
    "### Tasks: \n",
    "* Implement LDA using the gensim library to identify main topics in the articles.\n",
    "* Analyze the topics to understand potential areas of bias."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom LDA",
   "id": "2cea2b41e9e9ebe8"
  },
  {
   "cell_type": "code",
   "id": "9afbd83dddece749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T04:20:45.343180Z",
     "start_time": "2024-11-19T04:20:44.977751Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "class CustomLDA:\n",
    "    def __init__(self, corpus, num_topics=50, alpha=0.01, beta=0.1, num_iterations=100):\n",
    "        self.corpus = corpus\n",
    "        self.num_topics = num_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_iterations = num_iterations\n",
    "        self.vocab = set(word for doc in corpus for word in doc)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.id_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.doc_topic_counts = np.zeros((len(corpus), num_topics))\n",
    "        self.topic_word_counts = np.zeros((num_topics, self.vocab_size))\n",
    "        self.topic_counts = np.zeros(num_topics)\n",
    "        self.word_assignments = []\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            word_assignments_doc = []\n",
    "            for word in doc:\n",
    "                word_id = self.word_to_id[word]\n",
    "                topic = random.randint(0, num_topics - 1)\n",
    "                word_assignments_doc.append(topic)\n",
    "                self.doc_topic_counts[doc_idx, topic] += 1\n",
    "                self.topic_word_counts[topic, word_id] += 1\n",
    "                self.topic_counts[topic] += 1\n",
    "            self.word_assignments.append(word_assignments_doc)\n",
    "\n",
    "    def sample_topic(self, doc_idx, word_idx, word_id):\n",
    "        current_topic = self.word_assignments[doc_idx][word_idx]\n",
    "        self.doc_topic_counts[doc_idx, current_topic] -= 1\n",
    "        self.topic_word_counts[current_topic, word_id] -= 1\n",
    "        self.topic_counts[current_topic] -= 1\n",
    "        topic_probs = (self.doc_topic_counts[doc_idx] + self.alpha) * \\\n",
    "                      (self.topic_word_counts[:, word_id] + self.beta) / \\\n",
    "                      (self.topic_counts + self.beta * self.vocab_size)\n",
    "        topic_probs /= topic_probs.sum()\n",
    "        new_topic = np.random.choice(self.num_topics, p=topic_probs)\n",
    "        self.doc_topic_counts[doc_idx, new_topic] += 1\n",
    "        self.topic_word_counts[new_topic, word_id] += 1\n",
    "        self.topic_counts[new_topic] += 1\n",
    "        return new_topic\n",
    "\n",
    "    def train(self):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{self.num_iterations}\")\n",
    "            for doc_idx, doc in enumerate(self.corpus):\n",
    "                for word_idx, word in enumerate(doc):\n",
    "                    word_id = self.word_to_id[word]\n",
    "                    new_topic = self.sample_topic(doc_idx, word_idx, word_id)\n",
    "                    self.word_assignments[doc_idx][word_idx] = new_topic\n",
    "\n",
    "    def print_topics(self, top_n=5):\n",
    "        for topic_id in range(self.num_topics):\n",
    "            word_probs = self.topic_word_counts[topic_id] / self.topic_counts[topic_id]\n",
    "            top_word_ids = np.argsort(word_probs)[::-1][:top_n]\n",
    "            top_words = [self.id_to_word[word_id] for word_id in top_word_ids]\n",
    "            print(f\"Topic {topic_id}: {', '.join(top_words)}\")\n",
    "\n",
    "    def get_document_topics(self, top_n=5):\n",
    "        doc_topic_probs = (self.doc_topic_counts + self.alpha) / \\\n",
    "                          (self.doc_topic_counts.sum(axis=1)[:, None] + self.alpha * self.num_topics)\n",
    "        top_topics_per_doc = []\n",
    "        for doc_idx in range(len(self.corpus)):\n",
    "            topic_probs = doc_topic_probs[doc_idx]\n",
    "            top_topic_ids = np.argsort(topic_probs)[::-1][:top_n]\n",
    "            top_topic_probs = topic_probs[top_topic_ids]\n",
    "            top_topics_per_doc.append([(topic_id, prob) for topic_id, prob in zip(top_topic_ids, top_topic_probs)])\n",
    "        return top_topics_per_doc\n",
    "\n",
    "# Example usage\n",
    "lda = CustomLDA(ds_cleaned['lemmatized_tokens'], num_topics=2, num_iterations=2)\n",
    "lda.train()\n",
    "\n",
    "# Print top words for each topic\n",
    "# Print the top words for each topic\n",
    "print(\"\\nIdentified Topics:\")\n",
    "lda.print_topics(top_n=5)\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 72\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m top_topics_per_doc\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m lda \u001B[38;5;241m=\u001B[39m CustomLDA(\u001B[43mds_cleaned\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlemmatized_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m], num_topics\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, num_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     73\u001B[0m lda\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Print top words for each topic\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Print the top words for each topic\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ds_cleaned' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "db28ed670b87434",
   "metadata": {},
   "source": [
    "# Step 4: Bias Detection Model Development\n",
    "### Prerequisites:\n",
    "* Understanding of machine learning algorithms (e.g., logistic regression, random forests)\n",
    "* Knowledge of deep learning concepts and libraries (e.g., TensorFlow or PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173d853a65b0a39",
   "metadata": {},
   "source": [
    "### Logistic Regression Model for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "id": "532bc3ebab5f79cc",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model using the training data (TF-IDF features)\n",
    "logreg_model.fit(tfidf_features[:train_size], y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg_model.predict(tfidf_features[train_size:])\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report for detailed evaluation\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8983297c1e85315",
   "metadata": {},
   "source": [
    "### Neural Network Model for Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8b4a5705a94559b",
   "metadata": {},
   "source": [
    "# Install necessary libraries (run these commands separately in a notebook cell)\n",
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ensure TensorFlow is working\n",
    "print(tf.__version__)\n",
    "\n",
    "# Convert labels to numeric format\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Pad the sequences to make them of equal length (make sure tfidf_features is available)\n",
    "max_len = 100  # Set a maximum length for padding\n",
    "\n",
    "# Ensure tfidf_features is a 2D array or matrix of TF-IDF features\n",
    "X_train_padded = pad_sequences(tfidf_features[:train_size], maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(tfidf_features[train_size:], maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Build the neural network model\n",
    "nn_model = Sequential()\n",
    "\n",
    "# Add an embedding layer (if using word embeddings, else skip if TF-IDF features are used directly)\n",
    "# Embedding layer is typically not used with TF-IDF; remove if unnecessary\n",
    "nn_model.add(Embedding(input_dim=tfidf_features.shape[1], output_dim=100))\n",
    "\n",
    "# Add a spatial dropout layer\n",
    "nn_model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Add an LSTM layer\n",
    "nn_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Add a dense output layer with a sigmoid activation function (for binary classification)\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test_encoded))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = nn_model.evaluate(X_test_padded, y_test_encoded)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, print the training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9592f6c01a08d660",
   "metadata": {},
   "source": [
    "# Step 5: Token-level Bias Identification\n",
    "### Tasks:\n",
    "* Use the \"2-Token-Classification\" folder from the BEAD dataset.\n",
    "* Implement a BiLSTM-CRF model for identifying biased words or phrases within the text.\n",
    "* Train and evaluate the token-level bias detection model."
   ]
  },
  {
   "cell_type": "code",
   "id": "6a41bc67587147a6",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "# Convert the tokens to sequences using Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(df['lemmatized_text'])  # Assuming 'lemmatized_text' is the column with processed text\n",
    "\n",
    "# Convert text data into sequences of integers\n",
    "X_sequences = tokenizer.texts_to_sequences(df['lemmatized_text'])\n",
    "\n",
    "# Pad the sequences to ensure they have the same length\n",
    "max_len = 100  # You can adjust this based on your data\n",
    "X_padded = pad_sequences(X_sequences, padding='post', maxlen=max_len)\n",
    "\n",
    "# Encode labels (assume binary classification, modify if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Build the BiLSTM Model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Create the BiLSTM model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add an embedding layer (using the tokenizer's word index)\n",
    "model.add(layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\n",
    "\n",
    "# Add a BiLSTM layer\n",
    "model.add(layers.Bidirectional(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "# Add a Dense layer for output prediction (binary classification)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# 5. Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49c65f26564da24d",
   "metadata": {},
   "source": [
    "# Step 6: Bias Categorization\n",
    "### Tasks:\n",
    "* Develop a multi-label classification model to categorize different types of bias (e.g., gender, racial, political).\n",
    "* Train the model using the data from the \"3-Aspects\" folder.\n",
    "* Evaluate the model's performance in categorizing bias types."
   ]
  },
  {
   "cell_type": "code",
   "id": "2a48f2a9c8f47406",
   "metadata": {},
   "source": [
    "!pip install tensorflow-addons==0.15.0\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1e9f0692cc1072e",
   "metadata": {},
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_addons as tfa  # Ensure to import tensorflow_addons for CRF\n",
    "\n",
    "# Define the input layer with shape corresponding to the padded sequences\n",
    "input_layer = layers.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Add an embedding layer that uses the tokenizer's word index\n",
    "embedding_layer = layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_train.shape[1])(input_layer)\n",
    "\n",
    "# Add a BiLSTM layer to process the sequence data\n",
    "bilstm_layer = layers.Bidirectional(layers.LSTM(100, return_sequences=True))(embedding_layer)\n",
    "\n",
    "# Add a CRF layer for sequence labeling\n",
    "crf_layer = tfa.layers.CRF(len(token_labels))  # Ensure to pass the correct number of tags/labels\n",
    "\n",
    "# Apply CRF layer on top of the BiLSTM output\n",
    "output_layer = crf_layer(bilstm_layer)\n",
    "\n",
    "# Build the model by specifying input and output layers\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model using Adam optimizer, CRF loss, and CRF accuracy metrics\n",
    "model.compile(optimizer='adam', loss=crf_layer.loss, metrics=[crf_layer.accuracy])\n",
    "\n",
    "# Display the model summary to check the architecture\n",
    "model.summary()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
